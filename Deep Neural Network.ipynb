{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous post, we have classified the MNIST dataset using softmax regression. We got nearly 90% accuracy for both training and testing data which is very bad for production. In this notebook, let's build a deeper neural network and see if it can helps improving the classification accuracy. Specifically we will build a L layer neural network with L-1 hidden layers.\n",
    "\n",
    "Let's first do some preprocessing and prepare our data,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples = 31500\n",
      "Number of testing examples = 10500\n",
      "Number of features = 784\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "\n",
    "data = pd.read_csv(\"data/MNIST_train.csv\").values\n",
    "\n",
    "X_data = data[:, 1:]\n",
    "Y_data = data[:, [0]]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_data, Y_data, random_state = 0)\n",
    "\n",
    "print(\"Number of training examples = \" + str(X_train.shape[0]) )\n",
    "print(\"Number of testing examples = \" + str(X_test.shape[0]) )\n",
    "print(\"Number of features = \" +  str(X_train.shape[1]) )\n",
    "\n",
    "index = 10\n",
    "sampleImg = np.reshape(X_train[index, :], [28, 28])\n",
    "\n",
    "ax = plt.imshow(sampleImg)\n",
    "plt.title(\"label is \" + str(Y_train[index, :]))\n",
    "\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_matrix(Y, num_class):\n",
    "    \"\"\"\n",
    "    Convert an array of Y to its one_hot_matrix\n",
    "    \n",
    "    Arguments:\n",
    "    Y -- array (number of examples, 1)\n",
    "    num_class -- num of classes\n",
    "    \n",
    "    Return:\n",
    "    Y_one_hot -- (number of examples, num_class)\n",
    "    \"\"\"\n",
    "    Y_one_hot = np.zeros((Y.shape[0], num_class))\n",
    "    Y_one_hot[np.arange(Y.shape[0]), Y.T] = 1\n",
    "    \n",
    "    return Y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Letter 9 converted to [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "Y_train_one_hot = one_hot_matrix(Y_train, 10)\n",
    "Y_test_one_hot = one_hot_matrix(Y_test, 10)\n",
    "\n",
    "index = 2\n",
    "print(\"Letter \" + str(Y_train[index, 0]) + \" converted to \" + str(Y_train_one_hot[index, :]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize parameters\n",
    "Our model structure is [LINEAR -> RELU] X (L-1) -> LINEAR -> SOFTMAX, i.e., it has L-1 hidden layers using a ReLU activation function followed by an output layer with a softmax activation function.\n",
    "\n",
    "We will store $n^{[l]}$, the number of units in different layers, in a variable layer_dims. For example, layer_dims = $[1000, 100, 10]$ means we have $1000$ input features, one hidden layer with $100$ units and an output layer with $10$ units. Thus, the shape of $W_1$ is $(1000, 100)$, $b_1$ is $(1, 100)$, $W_2$ is $(100, 10)$, and $b_2$ is $(1, 10)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python list containing the dimensions of each layer in our network.\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing parameters \"W1\", \"b1\", ... , \"WL\", \"bL\"\n",
    "                  Wl -- weight matrix of shape (layer_dims[l-1], layer_dims[l])\n",
    "                  bl -- bias matrix of shape (1, layer_dims[l])\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)        # number of layers + 1\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l-1], layer_dims[l]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((1, layer_dims[l]))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (num of examples, size of previous layer)\n",
    "    W -- weight matrix (size of previous layer, size of current layer)\n",
    "    b -- bias vector (1, size of current layer)\n",
    "    \n",
    "    Return: \n",
    "    Z -- (num of examples, size of current layer)\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = np.dot(A, W) + b\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    assert(Z.shape == (X.shape[0], W.shape[1]) )\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_max(Z):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Z -- (num of examples, size of current layer (or number of classes))\n",
    "    \n",
    "    Return:\n",
    "    A -- softmax matrix (m, size of current layer (or number of classes)) \n",
    "    \"\"\"\n",
    "    exp_Z = np.exp(Z)\n",
    "    A = exp_Z / exp_Z.sum(axis = 1, keepdims = True) # sum along columns\n",
    "    \n",
    "    assert(A.shape == Z.shape )\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Z -- (num of examples, size of current layer)\n",
    "    \n",
    "    Return:\n",
    "    A -- relu: 0 for Z < 0 or Z for Z >=0\n",
    "    \"\"\"\n",
    "    A = Z\n",
    "    \n",
    "    return A, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR -> ACTIVATION layer\n",
    "    \n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (num of examples, size of previous layer)\n",
    "    W -- weight matrix of current layer: (size of previous layer, size of current layer)\n",
    "    b -- bias matrix: (1, size of current layer)\n",
    "    activation -- \"relu\" or \"softmax\"\n",
    "    \"\"\"\n",
    "    \n",
    "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "    \n",
    "    if activation == \"relu\":    \n",
    "        A = relu(Z)\n",
    "    elif activation == \"softmax\":\n",
    "        A = softmax(Z)\n",
    "        \n",
    "    assert(A.shape == (A.prev.shape[0], W.shape[1]))\n",
    "    cache = linear_cache, Z\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for ([LINEAR]->RELU)*(L-1) -> LINEAR -> softmax\n",
    "    \n",
    "    Argument:\n",
    "    X -- input data matrix (num of examples, num of features)\n",
    "    parameters -- output of initialize_parameters()\n",
    "    \n",
    "    Return:\n",
    "    AL -- last post-activation value\n",
    "    caches -- caches for (L - 1) layers\n",
    "    \"\"\"\n",
    "    \n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2      # number of layers\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], \"relu\")\n",
    "        caches.append(cache)\n",
    "        \n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], \"softmax\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape[0] == X.shape[0])\n",
    "    \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cross_entropy_loss(A, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    A -- prediction matrix of shape   (num of examples, num of class)\n",
    "    Y -- one-hot matrix of true class (num of examples, num of class)\n",
    "    Return:\n",
    "    lost (scalar)\n",
    "    \"\"\"\n",
    "    m = A.shape[0]\n",
    "    return -(Y * np.log(A)).sum() / m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Backward propagation\n",
    "\n",
    "### 2.1 - Linear backward\n",
    "\n",
    "For layer $l$, the linear part is: $Z^{[l]} = A^{[l-1]} W^{[l]} + b^{[l]}$ (followed by an activation).\n",
    "\n",
    "Suppose you have already calculated the derivative $dZ^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}$. We want to get $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$.\n",
    "\n",
    "For one training example and unit $j$, we have $z_j = \\sum\\limits_{i = 1}^{size of previous layer} a_i w_{i, j} + b_j$, then,\n",
    "\n",
    "$$dw_{i, j} = dz_j \\frac{\\partial z_j}{\\partial dw_{i, j}} = dz_j * a_i $$\n",
    "$$db_j = dz_j \\frac{\\partial z_j}{\\partial db_j} = dz_j $$\n",
    "$$da_i = dz_j w_{i, j} $$\n",
    "\n",
    "The three outputs $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$ are computed using the input $dZ^{[l]}$:\n",
    "$$ dW^{[l]} =  \\frac{1}{m} A^{[l-1]} dZ^{[l]T}  $$\n",
    "$$ db^{[l]} = \\frac{1}{m} \\ dZ^{[l]}.sum(axis = 0) $$\n",
    "$$ dA^{[l-1]} = dZ^{[l]} W^{[l] T} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement linear backward propagation for 1 layer\n",
    "    \n",
    "    Arguments:\n",
    "    dZ\n",
    "    cache -- (A_prev, W, b) from the forward propagation in the current layer\n",
    "    \n",
    "    Return:\n",
    "    dA_prev\n",
    "    dW\n",
    "    db\n",
    "    \"\"\"\n",
    "    \n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[0]\n",
    "    \n",
    "    dW = 1 / m * np.dot(A_prev, dZ.T)\n",
    "    db = 1 / m * np.sum(dZ, axis = 0)\n",
    "    dA_prev = np.dot(dZ, W.T)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Activation backward\n",
    "Compute $dZ^{[l]} = dA^{[l]} * g'(Z^{[l]})$, where $g(.)$ is the activation function. Let's just do this for relu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, Z):\n",
    "    dZ = dA * (Z > 0)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Linear-activation backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, Z) we store for computing backward propagation efficiently\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, Z = cache\n",
    "    \n",
    "    dZ = relu_backward(dA, Z)\n",
    "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SOFTMAX group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- one-hot true \"label\" vector\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[0]\n",
    "    \n",
    "    # Lth layer (SOFTMAX -> LINEAR) gradients.  \n",
    "    dZ = AL - Y\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_backward(dZ, cache(L))\n",
    "    \n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], caches[l], \"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
