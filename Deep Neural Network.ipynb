{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous post, we have classified the MNIST dataset using softmax regression. We got nearly 90% accuracy for both training and testing data which is pretty bad for production. In this notebook, let's build a deeper neural network and see if it can helps improving the classification accuracy. Specifically we will build a L layer neural network with L-1 hidden layers.\n",
    "\n",
    "Let's first do some preprocessing and prepare our data,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples = 31500\n",
      "Number of testing examples = 10500\n",
      "Number of features = 784\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "\n",
    "data = pd.read_csv(\"data/MNIST_train.csv\").values\n",
    "\n",
    "X_data = data[:, 1:]\n",
    "Y_data = data[:, [0]]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_data, Y_data, random_state = 0)\n",
    "\n",
    "print(\"Number of training examples = \" + str(X_train.shape[0]) )\n",
    "print(\"Number of testing examples = \" + str(X_test.shape[0]) )\n",
    "print(\"Number of features = \" +  str(X_train.shape[1]) )\n",
    "\n",
    "index = 10\n",
    "sampleImg = np.reshape(X_train[index, :], [28, 28])\n",
    "\n",
    "ax = plt.imshow(sampleImg)\n",
    "plt.title(\"label is \" + str(Y_train[index, :]))\n",
    "\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_matrix(Y, num_class):\n",
    "    \"\"\"\n",
    "    Convert an array of Y to its one_hot_matrix\n",
    "    \n",
    "    Arguments:\n",
    "    Y -- array (number of examples, 1)\n",
    "    num_class -- num of classes\n",
    "    \n",
    "    Return:\n",
    "    Y_one_hot -- (number of examples, num_class)\n",
    "    \"\"\"\n",
    "    Y_one_hot = np.zeros((Y.shape[0], num_class))\n",
    "    Y_one_hot[np.arange(Y.shape[0]), Y.T] = 1\n",
    "    \n",
    "    return Y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Letter 9 converted to [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "Y_train_one_hot = one_hot_matrix(Y_train, 10)\n",
    "Y_test_one_hot = one_hot_matrix(Y_test, 10)\n",
    "\n",
    "index = 2\n",
    "print(\"Letter \" + str(Y_train[index, 0]) + \" converted to \" + str(Y_train_one_hot[index, :]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/NN_cg.jpeg\">\n",
    "\n",
    "## 1. Initialize parameters\n",
    "Our model structure is [LINEAR -> RELU] X (L-1) -> LINEAR -> SOFTMAX, i.e., it has L-1 hidden layers using a ReLU activation function followed by an output layer with a softmax activation function.\n",
    "\n",
    "We will store $n^{[l]}$, the number of units in different layers, in a variable layer_dims. For example, layer_dims = $[1000, 100, 10]$ means we have $1000$ input features, one hidden layer with $100$ units and an output layer with $10$ units. Thus, the shape of $W_1$ is $(1000, 100)$, $b_1$ is $(1, 100)$, $W_2$ is $(100, 10)$, and $b_2$ is $(1, 10)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python list containing the dimensions of each layer in our network.\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing parameters \"W1\", \"b1\", ... , \"WL\", \"bL\"\n",
    "                  Wl -- weight matrix of shape (layer_dims[l-1], layer_dims[l])\n",
    "                  bl -- bias matrix of shape (1, layer_dims[l])\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    parameters = {}\n",
    "    L_plus_one = len(layer_dims)        # number of layers + 1\n",
    "    \n",
    "    for l in range(1, L_plus_one):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l-1], layer_dims[l]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((1, layer_dims[l]))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (num of examples, size of previous layer)\n",
    "    W -- weight matrix (size of previous layer, size of current layer)\n",
    "    b -- bias vector (1, size of current layer)\n",
    "    \n",
    "    Return: \n",
    "    Z -- (num of examples, size of current layer)\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = np.dot(A, W) + b\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    assert(Z.shape == (A.shape[0], W.shape[1]) )\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_max(Z):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Z -- (num of examples, size of current layer (or number of classes))\n",
    "    \n",
    "    Return:\n",
    "    A -- softmax matrix (m, size of current layer (or number of classes)) \n",
    "    \"\"\"\n",
    "    exp_Z = np.exp(Z)\n",
    "    A = exp_Z / exp_Z.sum(axis = 1, keepdims = True) # sum along columns\n",
    "    \n",
    "    assert(A.shape == Z.shape )\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Z -- (num of examples, size of current layer)\n",
    "    \n",
    "    Return:\n",
    "    A -- relu: 0 for Z < 0 or Z for Z >=0\n",
    "    \"\"\"\n",
    "    A = Z * (Z > 0)\n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR -> ACTIVATION layer\n",
    "    \n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (num of examples, size of previous layer)\n",
    "    W -- weight matrix of current layer: (size of previous layer, size of current layer)\n",
    "    b -- bias matrix: (1, size of current layer)\n",
    "    activation -- \"relu\" or \"softmax\"\n",
    "    \"\"\"\n",
    "    \n",
    "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "    \n",
    "    if activation == \"relu\":    \n",
    "        A = relu(Z)\n",
    "    elif activation == \"softmax\":\n",
    "        A = soft_max(Z)\n",
    "        \n",
    "    assert(A.shape == (A_prev.shape[0], W.shape[1]))\n",
    "    cache = linear_cache, Z\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for ([LINEAR]->RELU)*(L-1) -> LINEAR -> softmax\n",
    "    \n",
    "    Argument:\n",
    "    X -- input data matrix (num of examples, num of features)\n",
    "    parameters -- output of initialize_parameters()\n",
    "    \n",
    "    Return:\n",
    "    AL -- last post-activation value\n",
    "    caches -- caches for all L layers, index from 0,...,L-1\n",
    "    \"\"\"\n",
    "    \n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2      # number of layers\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], \"relu\")\n",
    "        caches.append(cache)\n",
    "        \n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], \"softmax\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape[0] == X.shape[0])\n",
    "    assert(len(caches) == L)\n",
    "    \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cross_entropy_loss(A, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    A -- prediction matrix of shape   (num of examples, num of class)\n",
    "    Y -- one-hot matrix of true class (num of examples, num of class)\n",
    "    Return:\n",
    "    lost (scalar)\n",
    "    \"\"\"\n",
    "    m = A.shape[0]\n",
    "    return -(Y * np.log(A)).sum() / m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Backward propagation\n",
    "\n",
    "### 2.1 - Linear backward\n",
    "\n",
    "For layer $l$, the linear part is: $Z^{[l]} = A^{[l-1]} W^{[l]} + b^{[l]}$ (followed by an activation).\n",
    "\n",
    "Suppose you have already calculated the derivative $dZ^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}$. We want to get $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$.\n",
    "\n",
    "For one training example and unit $j$, we have $z_j = \\sum\\limits_{i = 1}^{size of previous layer} a_i w_{i, j} + b_j$, then,\n",
    "\n",
    "$$dw_{i, j} = dz_j \\frac{\\partial z_j}{\\partial dw_{i, j}} = dz_j * a_i $$\n",
    "$$db_j = dz_j \\frac{\\partial z_j}{\\partial db_j} = dz_j $$\n",
    "$$da_i = dz_j w_{i, j} $$\n",
    "\n",
    "The three outputs $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$ are computed using the input $dZ^{[l]}$:\n",
    "$$ dW^{[l]} =  \\frac{1}{m} A^{[l-1]T} dZ^{[l]}  $$\n",
    "$$ db^{[l]} = \\frac{1}{m} \\ dZ^{[l]}.sum(axis = 0) $$\n",
    "$$ dA^{[l-1]} = dZ^{[l]} W^{[l] T} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement linear backward propagation for 1 layer\n",
    "    \n",
    "    Arguments:\n",
    "    dZ\n",
    "    cache -- (A_prev, W, b) from the forward propagation in the current layer\n",
    "    \n",
    "    Return:\n",
    "    dA_prev\n",
    "    dW\n",
    "    db\n",
    "    \"\"\"\n",
    "    linear_cache, Z = cache\n",
    "    A_prev, W, b = linear_cache\n",
    "    m = A_prev.shape[0]\n",
    "    \n",
    "    dW = 1 / m * np.dot(A_prev.T, dZ)\n",
    "    db = 1 / m * np.sum(dZ, axis = 0)\n",
    "    dA_prev = np.dot(dZ, W.T)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Activation backward\n",
    "Compute $dZ^{[l]} = dA^{[l]} * g'(Z^{[l]})$, where $g(.)$ is the activation function. Let's just do this for relu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, Z):\n",
    "    dZ = dA * (Z > 0)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Linear-activation backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, Z) we store for computing backward propagation efficiently\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, Z = cache\n",
    "    \n",
    "    dZ = relu_backward(dA, Z)\n",
    "    dA_prev, dW, db = linear_backward(dZ, cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SOFTMAX group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- one-hot true \"label\" vector\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[0]\n",
    "    \n",
    "    # Lth layer (SOFTMAX -> LINEAR) gradients.  \n",
    "    dZ = AL - Y\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_backward(dZ, caches[L - 1])\n",
    "    \n",
    "    # Loop from l=L-1 to l=1\n",
    "    for l in reversed(range(1, L)):\n",
    "        grads[\"dA\" + str(l-1)], grads[\"dW\" + str(l)], grads[\"db\" + str(l)] = linear_activation_backward(grads[\"dA\" + str(l)], caches[l-1])\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "    \n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):#lr was 0.009\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SOFTMAX.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (number of examples, number of features)\n",
    "    Y -- one-hot true \"label\" vector\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    parameters = initialize_parameters(layers_dims)\n",
    "\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SOFTMAX.\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        \n",
    "        # Compute cost.\n",
    "        cost = compute_cross_entropy_loss(AL, Y)\n",
    "    \n",
    "        # Backward propagation.\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    " \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 2.301802\n",
      "Cost after iteration 100: 1.105076\n",
      "Cost after iteration 200: 0.532143\n",
      "Cost after iteration 300: 0.418412\n",
      "Cost after iteration 400: 0.370337\n",
      "Cost after iteration 500: 0.342632\n",
      "Cost after iteration 600: 0.322993\n",
      "Cost after iteration 700: 0.307524\n",
      "Cost after iteration 800: 0.294639\n",
      "Cost after iteration 900: 0.283207\n",
      "Cost after iteration 1000: 0.272946\n",
      "Cost after iteration 1100: 0.263549\n",
      "Cost after iteration 1200: 0.254788\n",
      "Cost after iteration 1300: 0.246499\n",
      "Cost after iteration 1400: 0.238664\n",
      "Cost after iteration 1500: 0.231262\n",
      "Cost after iteration 1600: 0.224205\n",
      "Cost after iteration 1700: 0.217490\n",
      "Cost after iteration 1800: 0.211091\n",
      "Cost after iteration 1900: 0.205021\n",
      "Cost after iteration 2000: 0.199212\n",
      "Cost after iteration 2100: 0.193722\n",
      "Cost after iteration 2200: 0.188499\n",
      "Cost after iteration 2300: 0.183508\n",
      "Cost after iteration 2400: 0.178728\n",
      "Cost after iteration 2500: 0.174200\n",
      "Cost after iteration 2600: 0.169893\n",
      "Cost after iteration 2700: 0.165781\n",
      "Cost after iteration 2800: 0.161842\n",
      "Cost after iteration 2900: 0.158082\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "layers_dims = [784, 50, 10] #  4-layer model\n",
    "parameters = L_layer_model(X_train, Y_train_one_hot, layers_dims, learning_rate = 0.1, num_iterations = 3000, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, parameters):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- input data (m, num_feature)\n",
    "    \n",
    "    Return:\n",
    "    prediction -- softmax vector (1, num_feature)\n",
    "    \"\"\"\n",
    "    AL, _ = L_model_forward(X, parameters)\n",
    "    \n",
    "    prediction = np.argmax(AL, axis = 1)\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 95.663492%\n"
     ]
    }
   ],
   "source": [
    "# Training accuracy\n",
    "Y_train_hat = predict(X_train, parameters)\n",
    "\n",
    "m = X_train.shape[0]\n",
    "num_correct = m - np.count_nonzero((np.squeeze(Y_train) - Y_train_hat))\n",
    "\n",
    "print(\"Training accuracy: %f\" % float(num_correct / m * 100.0) + \"%\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy: 94.771429%\n"
     ]
    }
   ],
   "source": [
    "# Testing accuracy\n",
    "Y_test_hat = predict(X_test, parameters)\n",
    "\n",
    "m = X_test.shape[0]\n",
    "num_correct = m - np.count_nonzero((np.squeeze(Y_test) - Y_test_hat))\n",
    "\n",
    "print(\"Testing accuracy: %f\" % float(num_correct / m * 100.0) + \"%\" )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
